{"cells":[{"cell_type":"markdown","source":["# Chat bot interface"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"912812ee-d3e0-43d2-8dc5-f4b07bb68d0a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%pip install -U chromadb==0.3.22 langchain==0.0.164 transformers==4.29.0 accelerate==0.19.0 bitsandbytes"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d1bc4670-efa0-4152-b337-9b3bdf2b6762","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%run ./_resources/00-init $catalog=hive_metastore $db=dbdemos_llm"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"24d13018-b4b1-48f4-8821-1c33b8e44750","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["if len(get_available_gpus()) == 0:\n  Exception(\"Running dolly without GPU will be slow. We recommend you switch to a Single Node cluster with at least 1 GPU to properly run this demo.\")\n\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import Chroma\n\ngardening_vector_db_path = \"/dbfs\"+demo_path+\"/vector_db\"\nhf_embed = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\nchroma_db = Chroma(collection_name=\"gardening_docs\", embedding_function=hf_embed, persist_directory=gardening_vector_db_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7f8fc3c8-1128-4024-958f-d3fec1d29893","inputWidgets":{},"title":"Create our vector database connection for context"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\nfrom langchain import PromptTemplate\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.chains.question_answering import load_qa_chain\nfrom langchain.memory import ConversationSummaryBufferMemory\n\ndef build_qa_chain():\n  torch.cuda.empty_cache()\n  # Defining our prompt content.\n  # langchain will load our similar documents as {context}\n  template = \"\"\"You are a chatbot having a conversation with a human. Your are asked to generate a recipe based on ingredients available.\n  Given the following extracted parts of a long document and a question, answer the user question. If you don't know, say that you do not know. \n  \n  {context}\n\n  {chat_history}\n\n  {human_input}\n\n  Response:\n  \"\"\"\n  prompt = PromptTemplate(input_variables=['context', 'human_input', 'chat_history'], template=template)\n\n  # Increase max_new_tokens for a longer response\n  # Other settings might give better results! Play around\n  model_name = \"databricks/dolly-v2-7b\" # can use dolly-v2-3b, dolly-v2-7b or dolly-v2-12b for smaller model and faster inferences.\n  instruct_pipeline = pipeline(model=model_name, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\", \n                               return_full_text=True, max_new_tokens=256, top_p=0.95, top_k=50)\n  hf_pipe = HuggingFacePipeline(pipeline=instruct_pipeline)\n\n  # Add a summarizer to our memory conversation\n  # Let's make sure we don't summarize the discussion too much to avoid losing to much of the content\n\n  # Models we'll use to summarize our chat history\n  # We could use one of these models: https://huggingface.co/models?filter=summarization. facebook/bart-large-cnn gives great results, we'll use t5-small for memory\n  summarize_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\", device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True)\n  summarize_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\", padding_side=\"left\", model_max_length = 512)\n  pipe_summary = pipeline(\"summarization\", model=summarize_model, tokenizer=summarize_tokenizer) #, max_new_tokens=500, min_new_tokens=300\n  # langchain pipeline doesn't support summarization yet, we added it as temp fix in the companion notebook _resources/00-init \n  hf_summary = HuggingFacePipeline_WithSummarization(pipeline=pipe_summary)\n  #will keep 500 token and then ask for a summary. Removes prefix as our model isn't trained on specific chat prefix and can get confused.\n  memory = ConversationSummaryBufferMemory(llm=hf_summary, memory_key=\"chat_history\", input_key=\"human_input\", max_token_limit=500, human_prefix = \"\", ai_prefix = \"\")\n\n  # Set verbose=True to see the full prompt:\n  print(\"loading chain, this can take some time...\")\n  return load_qa_chain(llm=hf_pipe, chain_type=\"stuff\", prompt=prompt, verbose=True, memory=memory)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b79ab3a4-6efc-4875-a7ff-9f2762054cb3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["class ChatBot():\n  def __init__(self, db):\n    self.reset_context()\n    self.db = db\n\n  def reset_context(self):\n    self.sources = []\n    self.discussion = []\n    # Building the chain will load Dolly and can take some time depending on the model size and your GPU\n    self.qa_chain = build_qa_chain()\n    displayHTML(\"<h1>Hi! I'm a chat bot specialized in gardening. How Can I help you today?</h1>\")\n\n  def get_similar_docs(self, question, similar_doc_count):\n    return self.db.similarity_search(question, k=similar_doc_count)\n\n  def chat(self, question):\n    # Keep the last 3 discussion to search similar content\n    self.discussion.append(question)\n    similar_docs = self.get_similar_docs(\" \\n\".join(self.discussion[-3:]), similar_doc_count=2)\n    # Remove similar doc if they're already in the last questions (as it's already in the history)\n    similar_docs = [doc for doc in similar_docs if doc.metadata['source'] not in self.sources[-3:]]\n\n    result = self.qa_chain({\"input_documents\": similar_docs, \"human_input\": question})\n    # Cleanup the answer for better display:\n    answer = result['output_text'].capitalize()\n    result_html = f\"<p><blockquote style=\\\"font-size:24\\\">{question}</blockquote></p>\"\n    result_html += f\"<p><blockquote style=\\\"font-size:18px\\\">{answer}</blockquote></p>\"\n    #result_html += \"<p><hr/></p>\"\n    #for d in result[\"input_documents\"]:\n    #  source_id = d.metadata[\"source\"]\n    #  self.sources.append(source_id)\n    #  result_html += f\"<p><blockquote>{d.page_content}<br/>(Source: <a href=\\\"https://gardening.stackexchange.com/a/#{source_id}\\\">{source_id}</a>)</blockquote></p>\"\n    displayHTML(result_html)\n\nchat_bot = ChatBot(chroma_db)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"214d6ef1-0409-42cb-b766-2bae1e05baac","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Try asking for a recipe!"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4fffb2be-2ae1-4d5d-8d5e-1d45d4290881","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["chat_bot.chat(\"What is the best kind of soil to grow blueberries in?\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"852492bd-e747-440f-ae68-1d8442feb1e4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["chat_bot.chat(\"How much water should I give?\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1f20fa9f-58ec-49a7-be63-70dc800d12a0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Chefbot interface","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":3215166141061539,"dataframes":["_sqldf"]}},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
